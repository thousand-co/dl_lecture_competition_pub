{"cells":[{"cell_type":"markdown","metadata":{"id":"yN8Jq5a8J5HV"},"source":["### データの読み込み"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torchvision import transforms\n","from tqdm import tqdm_notebook as tqdm\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","import os\n","from tqdm import tqdm\n","\n","from src.datprep_i import DatPreprocess\n","from src.datasets import ThingsMEGDataset_aug1\n","#from src.models2 import BasicConvClassifier  # with glu\n","from src.utils import set_seed, set_lr, CosineScheduler"]},{"cell_type":"markdown","metadata":{"id":"MQpTXlwbKRdW"},"source":["### 自己教師あり学習の実装"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["import math\n","import torch.nn as nn\n","import torch.optim as optim\n","from einops.layers.torch import Rearrange\n","from einops import rearrange\n","\n","def fix_seed(seed=1234):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","\n","fix_seed(seed=42)"]},{"cell_type":"markdown","metadata":{},"source":["Attention"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, dim, heads, dim_head, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        dim : int\n","            入力データの次元数．埋め込み次元数と一致する．\n","        heads : int\n","            ヘッドの数．\n","        dim_head : int\n","            各ヘッドのデータの次元数．\n","        dropout : float\n","            Dropoutの確率(default=0.)．\n","        \"\"\"\n","        super().__init__()\n","\n","        self.dim = dim\n","        self.dim_head = dim_head\n","        inner_dim = dim_head * heads  # ヘッドに分割する前のQ, K, Vの次元数．self.dimと異なっても良い．\n","        project_out = not (heads == 1 and dim_head == dim)  # headsが1，dim_headがdimと等しければ通常のSelf-Attention\n","\n","        self.heads = heads\n","        self.scale = math.sqrt(dim_head)  # ソフトマックス関数を適用する前のスケーリング係数(dim_k)\n","\n","        self.attend = nn.Softmax(dim=-1)  # アテンションスコアの算出に利用するソフトマックス関数\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Q, K, Vに変換するための全結合層\n","        self.to_q = nn.Linear(in_features=dim, out_features=inner_dim)\n","        self.to_k = nn.Linear(in_features=dim, out_features=inner_dim)\n","        self.to_v = nn.Linear(in_features=dim, out_features=inner_dim)\n","\n","        # dim != inner_dimなら線形層を入れる，そうでなければそのまま出力\n","        self.to_out = nn.Sequential(\n","            nn.Linear(in_features=inner_dim, out_features=dim),\n","            nn.Dropout(dropout),\n","        ) if project_out else nn.Identity()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        B: バッチサイズ\n","        N: 系列長\n","        D: データの次元数(dim)\n","        \"\"\"\n","        B, N, D = x.size()\n","\n","        # 入力データをQ, K, Vに変換する\n","        # (B, N, dim) -> (B, N, inner_dim)\n","        q = self.to_q(x)\n","        k = self.to_k(x)\n","        v = self.to_v(x)\n","\n","        # Q, K, Vをヘッドに分割する\n","        # (B, N, inner_dim) -> (B, heads, N, dim_head)\n","        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n","        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n","        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n","\n","        # QK^T / sqrt(d_k)を計算する\n","        # (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\n","        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n","\n","        # ソフトマックス関数でスコアを算出し，Dropoutをする\n","        attn = self.attend(dots)\n","        attn = self.dropout(attn)\n","\n","        # softmax(QK^T / sqrt(d_k))Vを計算する\n","        # (B, heads, N, N) x (B, heads, N, dim_head) -> (B, heads, N, dim_head)\n","        out = torch.matmul(attn ,v)\n","\n","        # もとの形に戻す\n","        # (B, heads, N, dim_head) -> (B, N, dim)\n","        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n","\n","        # 次元が違っていればもとに戻して出力\n","        # 表現の可視化のためにattention mapも返すようにしておく\n","        return self.to_out(out), attn"]},{"cell_type":"markdown","metadata":{},"source":["Feed-forward network\n","=multi layer perceptron"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["class FFN(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        dim : int\n","            入力データの次元数．\n","        hidden_dim : int\n","            隠れ層の次元．\n","        dropout : float\n","            各全結合層の後のDropoutの確率(default=0.)．\n","        \"\"\"\n","        super().__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Linear(in_features=dim, out_features=hidden_dim),\n","            nn.GELU(), # Gaussian Error Linear Unit: ReLUに似た形状だがx=0で微分可能\n","            nn.Dropout(dropout),\n","            nn.Linear(in_features=hidden_dim, out_features=dim),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        (B, D) -> (B, D)\n","        B: バッチサイズ\n","        D: 次元数\n","        \"\"\"\n","        return self.net(x)"]},{"cell_type":"markdown","metadata":{},"source":["Transformer block"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, dim, heads, dim_head, mlp_dim, dropout):\n","        \"\"\"\n","        TransformerのEncoder Blockの実装．\n","\n","        Arguments\n","        ---------\n","        dim : int\n","            埋め込みされた次元数．PatchEmbedのembed_dimと同じ値．\n","        heads : int\n","            Multi-Head Attentionのヘッドの数．\n","        dim_head : int\n","            Multi-Head Attentionの各ヘッドの次元数．\n","        mlp_dim : int\n","            Feed-Forward Networkの隠れ層の次元数．\n","        dropout : float\n","            Droptou層の確率p．\n","        \"\"\"\n","        super().__init__()\n","\n","        self.attn_ln = nn.LayerNorm(dim)  # Attention前のLayerNorm\n","        self.attn = Attention(dim, heads, dim_head, dropout)\n","        self.ffn_ln = nn.LayerNorm(dim)  # FFN前のLayerNorm\n","        self.ffn = FFN(dim, mlp_dim, dropout)\n","\n","    def forward(self, x, return_attn=False):\n","        \"\"\"\n","        x: (B, N, dim)\n","        B: バッチサイズ\n","        N: 系列長\n","        dim: 埋め込み次元\n","        \"\"\"\n","        y, attn = self.attn(self.attn_ln(x))\n","        if return_attn:  # attention mapを返す（attention mapの可視化に利用）\n","            return attn\n","        x = y + x\n","        out = self.ffn(self.ffn_ln(x)) + x\n","\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["Patch embedding"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["class PatchEmbedding(nn.Module):\n","    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n","        \"\"\"\n","        入力画像をパッチごとに埋め込むための層．\n","\n","        Arguments\n","        ---------\n","        image_size : Tuple[int]\n","            入力画像のサイズ．\n","        patch_size : Tuple[int]\n","            各パッチのサイズ．\n","        in_channels : int\n","            入力画像のチャネル数．\n","        embed_dim : int\n","            埋め込み後の次元数．\n","        \"\"\"\n","        super().__init__()\n","\n","        image_height, image_width = image_size\n","        patch_height, patch_width = patch_size\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"パッチサイズは，入力画像のサイズを割り切れる必要があります．\"\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)  # パッチの数\n","        patch_dim = in_channels * patch_height * patch_width  # 各パッチを平坦化したときの次元数\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width),  # 画像をパッチに分割して平坦化\n","            nn.Linear(in_features=patch_dim, out_features=embed_dim),  # 埋め込みを行う\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        B: バッチサイズ\n","        C: 入力画像のチャネル数\n","        H: 入力画像の高さ\n","        W: 入力画像の幅\n","        \"\"\"\n","        return self.to_patch_embedding(x)  # (B, C, H, W) -> (B, num_patches, embed_dim)\n"]},{"cell_type":"markdown","metadata":{},"source":["Masked autoencoder"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["def random_indexes(size):\n","    \"\"\"\n","    パッチをランダムに並べ替えるためのindexを生成する関数．\n","\n","    Argument\n","    --------\n","    size : int\n","        入力されるパッチの数（系列長Nと同じ値）．\n","    \"\"\"\n","    forward_indexes = np.arange(size)  # 0からsizeまでを並べた配列を作成\n","    np.random.shuffle(forward_indexes)  # 生成した配列をシャッフルすることで，パッチの順番をランダムに決定\n","    backward_indexes = np.argsort(forward_indexes)  # 並べ替えたパッチをもとの順番に戻すためのidx\n","\n","    return forward_indexes, backward_indexes"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[],"source":["def take_indexes(sequences, indexes):\n","    \"\"\"\n","    パッチを並べ替えるための関数．\n","\n","    Argument\n","    --------\n","    sequences : torch.Tensor\n","        入力画像をパッチ分割したデータ．(B, N, dim)の形状をしている．\n","    indexes : np.ndarray\n","        並べ替えるために利用するindex．\n","        random_indexesで生成したforward_indexesかbackward_indexesが入ることが想定されている．\n","    \"\"\"\n","    # torch.gather: dim=1の時、indexで指定した位置の入力値を取ってきて並び替える\n","    return torch.gather(sequences, dim=1, index=indexes.unsqueeze(2).repeat(1, 1, sequences.shape[-1]))"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["class PatchShuffle(nn.Module):\n","    def __init__(self, ratio):\n","        # ratio: Encoderに入力しないパッチの割合\n","        super().__init__()\n","        self.ratio = ratio\n","\n","    def forward(self, patches):\n","        \"\"\"\n","        B: バッチサイズ\n","        N: 系列長（＝パッチの数）\n","        dim: 次元数（＝埋め込みの次元数）\n","        \"\"\"\n","        B, N, dim = patches.shape\n","        remain_N = int(N * (1 - self.ratio))  # Encoderに入力するパッチの数\n","\n","        indexes = [random_indexes(N) for _ in range(B)]  # バッチごとに異なる順番のindexを作る\n","        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # バッチを並べ替えるときのidx (B, N)\n","        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # 並べ替えたパッチをもとの順番に戻すためのidx  (B, N)\n","\n","        patches = take_indexes(patches, forward_indexes)  # パッチを並べ替える\n","        patches = patches[:, :remain_N, :]  # Encoderに入力するパッチを抽出\n","\n","        return patches, forward_indexes, backward_indexes"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["class MAE_Encoder(nn.Module):\n","    def __init__(self, image_size=[290, 290], patch_size=[29, 29], emb_dim=192, num_layer=12,\n","                 heads=3, dim_head=64, mlp_dim=192, mask_ratio=0.75, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","\n","        image_size : List[int]\n","            入力画像の大きさ．\n","        patch_size : List[int]\n","            各パッチの大きさ．\n","        emb_dim : int\n","            データを埋め込む次元の数．\n","        num_layer : int\n","            Encoderに含まれるBlockの数．\n","        heads : int\n","            Multi-Head Attentionのヘッドの数．\n","        dim_head : int\n","            Multi-Head Attentionの各ヘッドの次元数．\n","        mlp_dim : int\n","            Feed-Forward Networkの隠れ層の次元数．\n","        mask_ratio : float\n","            入力パッチのマスクする割合．\n","        dropout : float\n","            ドロップアウトの確率．\n","        \"\"\"\n","        super().__init__()\n","        img_height, img_width = image_size\n","        patch_height, patch_width = patch_size\n","        num_patches = (img_height // patch_height) * (img_width // patch_width)\n","\n","        self.cls_token = torch.nn.Parameter(torch.randn(1, 1, emb_dim))  # class tokenの初期化\n","        self.pos_embedding = torch.nn.Parameter(torch.randn(1, num_patches, emb_dim))  # positional embedding（学習可能にしている）\n","        self.shuffle = PatchShuffle(mask_ratio)\n","\n","        # 入力画像をパッチに分割する\n","        #self.patchify = PatchEmbedding(image_size, patch_size, 3, emb_dim)\n","        self.patchify = PatchEmbedding(image_size, patch_size, 1, emb_dim)  # changed\n","\n","        # Encoder（Blockを重ねる）\n","        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n","\n","        self.layer_norm = nn.LayerNorm(emb_dim)\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        torch.nn.init.normal_(self.cls_token, std=0.02)\n","        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n","\n","    def forward(self, img):\n","        # 1. 入力画像をパッチに分割して，positional embeddingする\n","        patches = self.patchify(img)\n","        patches = patches + self.pos_embedding\n","\n","        # 2. 分割したパッチをランダムに並べ替えて，必要なパッチのみ得る\n","        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n","\n","        # class tokenを結合\n","        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)\n","\n","        # 3. Encoderで入力データを処理する\n","        features = self.layer_norm(self.transformer(patches))\n","\n","        return features, backward_indexes"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["class MAE_Decoder(nn.Module):\n","    def __init__(self, image_size=[290, 290], patch_size=[29, 29], emb_dim=192, num_layer=4,\n","                 heads=3, dim_head=64, mlp_dim=192, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","\n","        image_size : List[int]\n","            入力画像の大きさ．\n","        patch_size : List[int]\n","            各パッチの大きさ．\n","        emb_dim : int\n","            データを埋め込む次元の数．\n","        num_layer : int\n","            Decoderに含まれるBlockの数．\n","        heads : int\n","            Multi-Head Attentionのヘッドの数．\n","        dim_head : int\n","            Multi-Head Attentionの各ヘッドの次元数．\n","        mlp_dim : int\n","            Feed-Forward Networkの隠れ層の次元数．\n","        dropout : float\n","            ドロップアウトの確率．\n","        \"\"\"\n","        super().__init__()\n","        img_height, img_width = image_size\n","        patch_height, patch_width = patch_size\n","        num_patches = (img_height // patch_height) * (img_width // patch_width)\n","\n","        self.mask_token = torch.nn.Parameter(torch.rand(1, 1, emb_dim))\n","        self.pos_embedding = torch.nn.Parameter(torch.rand(1, num_patches+1, emb_dim))\n","\n","        # Decoder(Blockを重ねる）\n","        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n","\n","        # 埋め込みされた表現から画像を復元するためのhead\n","        #self.head = torch.nn.Linear(emb_dim, 3 * patch_height * patch_width)\n","        self.head = torch.nn.Linear(emb_dim, 1 * patch_height * patch_width)  # changed\n","        # (B, N, dim)から(B, C, H, W)にreshapeするためのインスタンス\n","        self.patch2img = Rearrange(\"b (h w) (c p1 p2) -> b c (h p1) (w p2)\", p1=patch_height, p2=patch_width, h=img_height // patch_height)\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        torch.nn.init.normal_(self.mask_token, std=0.02)\n","        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n","\n","    def forward(self, features, backward_indexes):\n","        # 系列長\n","        T = features.shape[1]\n","\n","        # class tokenがある分backward_indexesの最初に0を追加する\n","        # .toはデバイスの変更でよく利用するが，tensorを渡すことでdtypeを変えることができる\n","        backward_indexes = torch.cat([torch.zeros(backward_indexes.shape[0], 1).to(backward_indexes), backward_indexes+1], dim=1)\n","\n","        # 1. mask_tokenを結合して並べ替える．\n","        # (B, N*(1-mask_ratio)+1, dim) -> (B, N+1, dim)\n","        features = torch.cat([features, self.mask_token.repeat(features.shape[0], backward_indexes.shape[1] - features.shape[1], 1)], dim=1)\n","        features = take_indexes(features, backward_indexes)\n","        features = features + self.pos_embedding\n","\n","        features = self.transformer(features)\n","\n","        # class tokenを除去する\n","        # (B, N+1, dim) -> (B, N, dim)\n","        features = features[:, 1:, :]\n","\n","        # 2. 画像を再構成する．\n","        # (B, N, dim) -> (B, N, 3 * patch_height * patch_width)\n","        patches = self.head(features)\n","\n","        # MAEではマスクした部分でのみ損失関数を計算するため，maskも一緒に返す\n","        mask = torch.zeros_like(patches)\n","        mask[:, T-1:] = 1  # cls tokenを含めていた分ずらしている\n","        mask = take_indexes(mask, backward_indexes[:, 1:] - 1)\n","\n","        img = self.patch2img(patches)\n","        mask = self.patch2img(mask)\n","\n","        return img, mask"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["class MAE_ViT(nn.Module):\n","    def __init__(self, image_size=[490, 490], patch_size=[49, 49], emb_dim=192,\n","                 enc_layers=12, enc_heads=3, enc_dim_head=64, enc_mlp_dim=768,\n","                 dec_layers=4, dec_heads=3, dec_dim_head=64, dec_mlp_dim=768,\n","                 mask_ratio=0.75, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        image_size : List[int]\n","            入力画像の大きさ．\n","        patch_size : List[int]\n","            各パッチの大きさ．\n","        emb_dim : int\n","            データを埋め込む次元の数．\n","        {enc/dec}_layers : int\n","            Encoder / Decoderに含まれるBlockの数．\n","        {enc/dec}_heads : int\n","            Encoder / DecoderのMulti-Head Attentionのヘッドの数．\n","        {enc/dec}_dim_head : int\n","            Encoder / DecoderのMulti-Head Attentionの各ヘッドの次元数．\n","        {enc/dec}_mlp_dim : int\n","            Encoder / DecoderのFeed-Forward Networkの隠れ層の次元数．\n","        mask_ratio : float\n","            入力パッチのマスクする割合．\n","        dropout : float\n","            ドロップアウトの確率．\n","        \"\"\"\n","        super().__init__()\n","\n","        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, enc_layers,\n","                                   enc_heads, enc_dim_head, enc_mlp_dim, mask_ratio, dropout)\n","        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, dec_layers,\n","                                   dec_heads, dec_dim_head, dec_mlp_dim, dropout)\n","\n","    def forward(self, img):\n","        features, backward_indexes = self.encoder(img)\n","        rec_img, mask = self.decoder(features, backward_indexes)\n","        return rec_img, mask\n","\n","    def get_last_selfattention(self, x):\n","        patches = self.encoder.patchify(x)\n","        patches = patches + self.encoder.pos_embedding\n","\n","        patches = torch.cat([self.encoder.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n","        for i, block in enumerate(self.encoder.transformer):\n","            if i < len(self.encoder.transformer) - 1:\n","                patches = block(patches)\n","            else:\n","                return block(patches, return_attn=True)"]},{"cell_type":"markdown","metadata":{},"source":["学習率スケジューラ"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["# cosine scheduler\n","class CosineScheduler:\n","    def __init__(self, epochs, lr, warmup_length=5):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        epochs : int\n","            学習のエポック数．\n","        lr : float\n","            学習率．\n","        warmup_length : int\n","            warmupを適用するエポック数．\n","        \"\"\"\n","        self.epochs = epochs\n","        self.lr = lr\n","        self.warmup = warmup_length\n","\n","    def __call__(self, epoch):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        epoch : int\n","            現在のエポック数．\n","        \"\"\"\n","        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n","        progress = np.clip(progress, 0.0, 1.0)\n","        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n","\n","        if self.warmup:\n","            lr = lr * min(1., (epoch+1) / self.warmup)\n","\n","        return lr"]},{"cell_type":"markdown","metadata":{},"source":["学習率変更関数"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["def set_lr(lr, optimizer):\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr"]},{"cell_type":"markdown","metadata":{"id":"hHOBi4auxuPR"},"source":["### Linear probing"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"1GeuhPBryfQa"},"outputs":[],"source":["aug='_baseline_resize'\n","transform_train=DatPreprocess(aug_sel=aug)\n","\n","# ------------------\n","#    Dataloader\n","# ------------------\n","loader_args = {\"batch_size\": 64, \"num_workers\": 4, \"pin_memory\": True}\n","\n","train_set = ThingsMEGDataset_aug1(\"train\", 'data', transform=transform_train)\n","val_set = ThingsMEGDataset_aug1(\"val\", 'data', transform=transform_train)\n","test_set = ThingsMEGDataset_aug1(\"test\", 'data', transform=transform_train)\n","\n","train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, **loader_args)\n","val_loader = torch.utils.data.DataLoader(val_set, shuffle=False, **loader_args)\n","test_loader = torch.utils.data.DataLoader(test_set, shuffle=False, **loader_args)\n"]},{"cell_type":"markdown","metadata":{},"source":["クラス分類器"]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[],"source":["class Classifier(nn.Module):\n","    def __init__(self, encoder: MAE_Encoder, num_classes=1854):\n","        super().__init__()\n","        self.cls_token = encoder.cls_token\n","        self.pos_embedding = encoder.pos_embedding\n","        self.patchify = encoder.patchify\n","        self.transformer = encoder.transformer\n","        self.layer_norm = encoder.layer_norm\n","        self.head = nn.Linear(self.pos_embedding.shape[-1], num_classes)\n","\n","    def forward(self, img):\n","        patches = self.patchify(img)\n","        patches = patches + self.pos_embedding  # positional embedding\n","\n","        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n","        features = self.layer_norm(self.transformer(patches))\n","        logits = self.head(features[:, 0])  # cls tokenのみを入力する\n","        return logits\n","\n","    def get_last_selfattention(self, x):\n","        patches = self.patchify(x)\n","        patches = patches + self.pos_embedding\n","\n","        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n","        for i, block in enumerate(self.transformer):\n","            if i < len(self.transformer) - 1:\n","                patches = block(patches)\n","            else:\n","                return block(patches, return_attn=True)"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[],"source":["# ハイパーパラメータの設定\n","config = {\n","    \"image_size\": [290, 290],\n","    \"patch_size\": [29, 29],\n","    \"emb_dim\": 128,\n","    \"enc_layers\": 12,\n","    \"enc_heads\": 4,\n","    \"enc_dim_head\": 128,\n","    \"enc_mlp_dim\": 128,\n","    \"dec_layers\": 4,\n","    \"dec_heads\": 4,\n","    \"dec_dim_head\": 64,\n","    \"dec_mlp_dim\": 64,\n","    \"mask_ratio\": 0.75,\n","    \"dropout\": 0.\n","}"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[],"source":["model_path_meg='./model/pretrain_meg_01.pth'\n","#model_path_img='./model/pretrain_01.pth'"]},{"cell_type":"code","execution_count":172,"metadata":{"id":"z8n5wVT-xvv1"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","pretrained_model = MAE_ViT(**config).to(device)\n","pretrained_model.load_state_dict(torch.load(model_path_meg, map_location=device))\n","\n","\n","encoder = pretrained_model.encoder\n","\n","# モデルの定義\n","model = Classifier(encoder).to(device)\n","\n","epochs = 50\n","lr = 0.0005\n","warmup_length = int(epochs*0.05)\n","#batch_size = 128\n","optimizer = optim.AdamW(model.head.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)  # 分類器部分のみ学習\n","scheduler = CosineScheduler(epochs, lr, warmup_length)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{},"source":["分類器の学習"]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[],"source":["class_path_best=\"./model/classifier_ens_best_01.pth\"\n","class_path_last=\"./model/classifier_ens_last_01.pth\""]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:37<00:00, 10.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 | train loss: 7.528 | train acc: 0.008 | val loss: 7.510 | val acc: 0.010\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:43<00:00,  9.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/50 | train loss: 7.489 | train acc: 0.015 | val loss: 7.487 | val acc: 0.014\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:36<00:00, 10.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/50 | train loss: 7.437 | train acc: 0.026 | val loss: 7.476 | val acc: 0.018\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:31<00:00, 11.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/50 | train loss: 7.393 | train acc: 0.037 | val loss: 7.469 | val acc: 0.021\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:31<00:00, 11.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/50 | train loss: 7.354 | train acc: 0.044 | val loss: 7.464 | val acc: 0.021\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:34<00:00, 10.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/50 | train loss: 7.317 | train acc: 0.050 | val loss: 7.460 | val acc: 0.023\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:38<00:00, 10.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/50 | train loss: 7.283 | train acc: 0.054 | val loss: 7.456 | val acc: 0.023\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:35<00:00, 10.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/50 | train loss: 7.251 | train acc: 0.057 | val loss: 7.453 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 10.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/50 | train loss: 7.221 | train acc: 0.060 | val loss: 7.451 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:31<00:00, 11.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/50 | train loss: 7.193 | train acc: 0.062 | val loss: 7.450 | val acc: 0.024\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 11.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11/50 | train loss: 7.167 | train acc: 0.064 | val loss: 7.449 | val acc: 0.024\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12/50 | train loss: 7.143 | train acc: 0.066 | val loss: 7.448 | val acc: 0.024\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13/50 | train loss: 7.120 | train acc: 0.070 | val loss: 7.448 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 14/50 | train loss: 7.099 | train acc: 0.072 | val loss: 7.447 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 15/50 | train loss: 7.079 | train acc: 0.073 | val loss: 7.447 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 16/50 | train loss: 7.060 | train acc: 0.076 | val loss: 7.448 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:30<00:00, 11.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 17/50 | train loss: 7.043 | train acc: 0.079 | val loss: 7.448 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:34<00:00, 10.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 18/50 | train loss: 7.027 | train acc: 0.082 | val loss: 7.449 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:31<00:00, 11.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 19/50 | train loss: 7.012 | train acc: 0.083 | val loss: 7.449 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:31<00:00, 11.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 20/50 | train loss: 6.998 | train acc: 0.086 | val loss: 7.450 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 21/50 | train loss: 6.985 | train acc: 0.088 | val loss: 7.450 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 22/50 | train loss: 6.973 | train acc: 0.090 | val loss: 7.451 | val acc: 0.024\n","\u001b[36mNew best.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 23/50 | train loss: 6.962 | train acc: 0.092 | val loss: 7.452 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 24/50 | train loss: 6.952 | train acc: 0.094 | val loss: 7.452 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 11.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 25/50 | train loss: 6.942 | train acc: 0.096 | val loss: 7.453 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 26/50 | train loss: 6.933 | train acc: 0.098 | val loss: 7.454 | val acc: 0.024\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 27/50 | train loss: 6.925 | train acc: 0.100 | val loss: 7.454 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:34<00:00, 10.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 28/50 | train loss: 6.917 | train acc: 0.101 | val loss: 7.455 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 29/50 | train loss: 6.910 | train acc: 0.103 | val loss: 7.456 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 30/50 | train loss: 6.903 | train acc: 0.105 | val loss: 7.456 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 11.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 31/50 | train loss: 6.897 | train acc: 0.105 | val loss: 7.457 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 32/50 | train loss: 6.892 | train acc: 0.107 | val loss: 7.457 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 33/50 | train loss: 6.887 | train acc: 0.108 | val loss: 7.457 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 10.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 34/50 | train loss: 6.882 | train acc: 0.109 | val loss: 7.458 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:36<00:00, 10.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 35/50 | train loss: 6.878 | train acc: 0.110 | val loss: 7.458 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [02:14<00:00,  7.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 36/50 | train loss: 6.875 | train acc: 0.111 | val loss: 7.458 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:34<00:00, 10.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 37/50 | train loss: 6.871 | train acc: 0.112 | val loss: 7.459 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 38/50 | train loss: 6.868 | train acc: 0.113 | val loss: 7.459 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:34<00:00, 10.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 39/50 | train loss: 6.866 | train acc: 0.113 | val loss: 7.459 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:35<00:00, 10.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 40/50 | train loss: 6.863 | train acc: 0.114 | val loss: 7.459 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 11.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 41/50 | train loss: 6.861 | train acc: 0.114 | val loss: 7.460 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:31<00:00, 11.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 42/50 | train loss: 6.860 | train acc: 0.114 | val loss: 7.460 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 43/50 | train loss: 6.858 | train acc: 0.115 | val loss: 7.460 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 44/50 | train loss: 6.857 | train acc: 0.115 | val loss: 7.460 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 10.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 45/50 | train loss: 6.856 | train acc: 0.116 | val loss: 7.460 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 46/50 | train loss: 6.856 | train acc: 0.116 | val loss: 7.460 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 10.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 47/50 | train loss: 6.855 | train acc: 0.116 | val loss: 7.460 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 10.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 48/50 | train loss: 6.855 | train acc: 0.116 | val loss: 7.460 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:32<00:00, 11.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 49/50 | train loss: 6.854 | train acc: 0.116 | val loss: 7.460 | val acc: 0.023\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100%|██████████| 1027/1027 [01:33<00:00, 10.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 50/50 | train loss: 6.854 | train acc: 0.116 | val loss: 7.460 | val acc: 0.023\n"]}],"source":["from torchmetrics import Accuracy\n","from termcolor import cprint\n","\n","max_val_acc = 0\n","accuracy = Accuracy(\n","    task=\"multiclass\", num_classes=train_set.num_classes, top_k=10\n",").to(device)\n","\n","for epoch in range(epochs):\n","    new_lr = scheduler(epoch)\n","    set_lr(new_lr, optimizer)\n","\n","    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n","\n","    scaler = torch.cuda.amp.GradScaler()  # added\n","    for x, t in tqdm(train_loader, desc=\"Train\"):\n","        x, t = x.to(device), t.to(device)\n","        with torch.cuda.amp.autocast():  # added\n","            pred = model(x)\n","            loss = criterion(pred, t)\n","\n","        train_loss.append(loss.item())\n","\n","        optimizer.zero_grad()\n","        #train_loss.backward()\n","        scaler.scale(loss).backward()\n","\n","        #optimizer.step()\n","        scaler.step(optimizer)\n","\n","        acc = accuracy(pred, t)\n","        train_acc.append(acc.item())\n","\n","        scaler.update()\n","\n","    with torch.no_grad():\n","        for x, t in val_loader:\n","            x, t = x.to(device), t.to(device)\n","            pred = model(x)\n","\n","            val_loss.append(criterion(pred, t).item())\n","            val_acc.append(accuracy(pred, t).item())\n","\n","    print(f\"Epoch {epoch+1}/{epochs} | train loss: {np.mean(train_loss):.3f} | train acc: {np.mean(train_acc):.3f} | val loss: {np.mean(val_loss):.3f} | val acc: {np.mean(val_acc):.3f}\")\n","    torch.save(model.state_dict(), class_path_last)\n","\n","    if np.mean(val_acc) > max_val_acc:\n","        cprint(\"New best.\", \"cyan\")\n","        torch.save(model.state_dict(), class_path_best)\n","        max_val_acc = np.mean(val_acc)\n","\n","torch.save(model.state_dict(), class_path_last)"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[],"source":["savedir=\"./outputs/2024-07-17/22-06-00/\""]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Validation: 100%|██████████| 257/257 [00:26<00:00,  9.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36mSubmission (16432, 1854) saved at ./outputs/2024-07-17/22-06-00/\u001b[0m\n"]}],"source":["model.load_state_dict(torch.load(class_path_best, map_location=device))\n","\n","preds = [] \n","model.eval()\n","for X in tqdm(test_loader, desc=\"Validation\"):        \n","    preds.append(model(X.to(device)).detach().cpu())\n","    \n","preds = torch.cat(preds, dim=0).numpy()\n","np.save(os.path.join(savedir, \"submission\"+aug), preds)\n","cprint(f\"Submission {preds.shape} saved at {savedir}\", \"cyan\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
